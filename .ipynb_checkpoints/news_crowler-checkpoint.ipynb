{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jqq2TSrwlR4b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import codecs\n",
    "import pickle\n",
    "import re\n",
    "import http\n",
    "from datetime import timedelta, date\n",
    "from urllib import error\n",
    "from urllib.request import urlopen, Request\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8MArqbA0tydQ"
   },
   "source": [
    "# Краулер по новостным статьям"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X7w-4w7vpYs2"
   },
   "outputs": [],
   "source": [
    "texts = {\"source\": [], \"author\": [], \"text\": [], \"date\": []}\n",
    "start_date = date(1999, 12, 31)\n",
    "end_date = date(2019, 1, 1)\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1) \\\n",
    "           AppleWebKit/537.36 (KHTML, like Gecko) \\\n",
    "           Chrome/41.0.2228.0 Safari/537.3\"}\n",
    "# fake headers, чтобы избежать ошибки 403 Forbidden при скачивании страниц\n",
    "\n",
    "\n",
    "def daterange(start_date, end_date):\n",
    "    # вспомогательная функция, упрощающая итерацию по датам\n",
    "    for n in range(int((end_date - start_date).days)):\n",
    "        yield start_date + timedelta(n)\n",
    "\n",
    "\n",
    "def download_page(pageUrl):\n",
    "    # функция, скачивающая страницу\n",
    "    req = Request(url=pageUrl, headers=headers)\n",
    "    try:\n",
    "        page = urlopen(req).read()\n",
    "    except (http.client.IncompleteRead) as e:\n",
    "        page = e.partial\n",
    "    except error.HTTPError as e:\n",
    "        if e.code == 404:\n",
    "            page = \"error\"\n",
    "    return page\n",
    "\n",
    "\n",
    "def get_index(source, date):\n",
    "    # функция, возвращающая индекс статей по дате\n",
    "    if source == \"Lenta.ru\":\n",
    "        link = \"https://lenta.ru/news/\" + date.strftime(\"%Y/%m/%d\")\n",
    "    if source == \"Полит.ру\":\n",
    "        link = \"https://polit.ru/news/\" + date.strftime(\"%Y/%m/%d\")\n",
    "    if source == \"Znak.com\":\n",
    "        link = \"https://www.znak.com/\" + date.strftime(\"%Y-%m-%d\")\n",
    "    if source == \"Интерфакс\":\n",
    "        link = \"https://www.interfax.ru/news/\" + date.strftime(\"%Y/%m/%d\")\n",
    "    return (link)\n",
    "\n",
    "\n",
    "def get_links_list(page, source):\n",
    "    # функция, достающая список ссылок на статьи из страницы\n",
    "    if source == \"Lenta.ru\":\n",
    "        links = page.find_all(\"div\", {\"class\": \"titles\"})\n",
    "        links = [\"https://lenta.ru\" + link.h3.a.get(\"href\") for link in links]\n",
    "    if source == \"Полит.ру\":\n",
    "        links = page.find_all(\"div\", {\"class\": \"news-full stop\"})\n",
    "        links = [\"https://polit.ru\" + link.h3.a.get(\"href\") for link in links]\n",
    "    if source == \"Znak.com\":\n",
    "        links = page.find_all(\"a\", {\"class\": \"pub\"})\n",
    "        links = [\"https://www.znak.com\" + link.get(\"href\") for link in links]\n",
    "    if source == \"Интерфакс\":\n",
    "        links = page.find(\"div\", {\"class\": \"an\"})\n",
    "        links = links.find_all(\"a\")\n",
    "        links = [\"https://www.interfax.ru/\" +\n",
    "                 link.get(\"href\") for link in links]\n",
    "    return (links)\n",
    "\n",
    "\n",
    "def is_error(page, source):\n",
    "    # функция, проверяющая, существует ли страница\n",
    "    if source == \"Lenta.ru\" and page.html.body[\"class\"] == \"page_error\":\n",
    "        return (True)\n",
    "    if source == \"Полит.ру\" and (page.html is None or \\\n",
    "                                 page.html.body is None or \\\n",
    "                                 page.html.body[\"class\"] == \"\"):\n",
    "        return (True)\n",
    "    if source == \"Znak.com\" and page.find(\"h1\", {\"class\": \"error404 x4\"}):\n",
    "        return (True)\n",
    "    if source == \"Интерфакс\" and page.html.head.title.string \\\n",
    "            == 'Документ не найден - \"Интерфакс\"':\n",
    "        return (True)\n",
    "    if page is None or str(page) == \"error\":\n",
    "        return (True)\n",
    "    return (False)\n",
    "\n",
    "\n",
    "def get_author(page, source):\n",
    "    # функция, достающая из html автора статьи - доступно только для Znak.com,\n",
    "    # в остальных СМИ авторы новостей анонимны\n",
    "    if source == \"Znak.com\":\n",
    "        try:\n",
    "            return (page.find(\"div\", {\"class\": \"credits__data\"}).get_text())\n",
    "        except:\n",
    "            return (\"Unknown\")\n",
    "    return (\"Unknown\")\n",
    "\n",
    "\n",
    "def get_text(page, source):\n",
    "    # функция, достающая готовый к лемматизации текст из html\n",
    "    if source == \"Lenta.ru\":\n",
    "        article = page.find(\"div\", {\"class\": \"b-text clearfix js-topic__text\"})\n",
    "        if article.script:\n",
    "            article.script.decompose()\n",
    "        for tag in article.find_all(\"div\"):\n",
    "            tag.decompose()\n",
    "        strings = article.strings\n",
    "        text = \" \".join(strings)\n",
    "    if source == \"Полит.ру\":\n",
    "        strings = page.find(\"div\",\n",
    "                            {\"class\": \"text doc js-mediator-article\"}).strings\n",
    "        text = \" \".join(strings)\n",
    "        text = re.sub(\"\\\\n\", \"\", text)\n",
    "    if source == \"Znak.com\":\n",
    "        article = page.find(\"article\")\n",
    "        if article.a:\n",
    "            article.a.decompose()\n",
    "        if article.script:\n",
    "            article.script.decompose()\n",
    "        for tag in article.find_all(\"div\"):\n",
    "            tag.decompose()\n",
    "        strings = article.strings\n",
    "        text = \" \".join(strings)\n",
    "        text.replace(\"\\xa0\", \" \")\n",
    "        text = text.strip()\n",
    "    if source == \"Интерфакс\":\n",
    "        strings = page.find(\"article\", {\"itemprop\": \"articleBody\"}).strings\n",
    "        text = \" \".join(strings)\n",
    "        text = re.sub(\"\\\\n\", \"\", text)\n",
    "    return (text)\n",
    "\n",
    "\n",
    "last_finished_date = start_date\n",
    "# если придётся прерывать скачивание\n",
    "\n",
    "\n",
    "def download_and_parse_texts(source,\n",
    "                             start=last_finished_date, end=end_date):\n",
    "    # функция, проходящая по всем статьям СМИ\n",
    "    # в период с 01.01.2000 по 31.12.2018\n",
    "    # и для каждой статьи записывающая текст,\n",
    "    # автора, дату и источник в словарь texts\n",
    "    global texts\n",
    "    current_month = start.month\n",
    "    for date_ in daterange(start + timedelta(days=1), end):\n",
    "        if date_.month != current_month:\n",
    "            current_month = date_.month\n",
    "            with open(\"texts.pkl\", \"wb\") as file:\n",
    "                pickle.dump(texts, file)\n",
    "        # сохраняем после каждого скачанного месяца\n",
    "        index = download_page(get_index(source, date_))\n",
    "        index_parsed = BeautifulSoup(index, \"html.parser\")\n",
    "        if not is_error(index_parsed, source):\n",
    "            try: \n",
    "                links_list = get_links_list(index_parsed, source)\n",
    "                for link in links_list:\n",
    "                    page = download_page(link)\n",
    "                    page_parsed = BeautifulSoup(page, \"html.parser\")\n",
    "                    if not is_error(page_parsed, source):\n",
    "                        texts[\"text\"].append(get_text(page_parsed, source))\n",
    "                        texts[\"author\"].append(get_author(page_parsed, source))\n",
    "                        texts[\"source\"].append(source)\n",
    "                        texts[\"date\"].append(date_)\n",
    "            except AttributeError:\n",
    "                continue\n",
    "        global last_finished_date\n",
    "        last_finished_date = date_\n",
    "        clear_output()\n",
    "        print(\"Last finished: {}\".format(last_finished_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#в случае, если скачивание придётся прервать\n",
    "with open(\"texts.pkl\", \"rb\") as file:\n",
    "    texts = pickle.load(file)\n",
    "last_finished_date = texts[\"date\"][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "BFNDuG8BdAds",
    "outputId": "1dbcaa3b-4f3b-4768-8b83-173f070c16d3"
   },
   "outputs": [],
   "source": [
    "# скачиваем тексты четырёх новостных СМИ\n",
    "download_and_parse_texts(\"Lenta.ru\", start=last_finished_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2YHpfMFERpEO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last finished: 2018-12-18\n"
     ]
    }
   ],
   "source": [
    "download_and_parse_texts(\"Полит.ру\", start=last_finished_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5TCoGH34RqBM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last finished: 2018-12-31\n"
     ]
    }
   ],
   "source": [
    "download_and_parse_texts(\"Znak.com\", start=last_finished_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CLpoiI8MRr1S"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last finished: 2018-12-29\n"
     ]
    }
   ],
   "source": [
    "download_and_parse_texts(\"Интерфакс\", start=last_finished_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-sJxVNqYRjVP"
   },
   "outputs": [],
   "source": [
    "# преобразуем получившийся словарь в датафрейм и сортируем по дате\n",
    "data = pd.DataFrame(texts)\n",
    "data = data.sort_values(by=\"date\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "colab_type": "code",
    "id": "7mJtn8xyfKiy",
    "outputId": "bb286404-da5a-4d5d-cc02-7b11f8789720"
   },
   "outputs": [],
   "source": [
    "# сохраняем в csv и как объект pickle\n",
    "data.to_csv(\"texts.csv\")\n",
    "pickle.dump(data, \"texts.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PvpjkQt8joHv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "курсовая_пичужкина_о_в.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
